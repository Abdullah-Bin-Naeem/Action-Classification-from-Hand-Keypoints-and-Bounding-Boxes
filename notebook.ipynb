{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SimpleVideoDataset(Dataset):\n",
    "    def __init__(self, video_frames_path, action_txt_path, xml_labels_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_frames_path (str): Path to the folder containing video frames.\n",
    "            action_txt_path (str): Path to the .txt file containing action labels.\n",
    "            xml_labels_path (str): Path to the .xml file containing annotations.\n",
    "        \"\"\"\n",
    "        self.video_frames_path = video_frames_path\n",
    "        self.action_txt_path = action_txt_path\n",
    "        self.xml_labels_path = xml_labels_path\n",
    "\n",
    "        # Action mapping\n",
    "        self.mapping = {\n",
    "            'take': 0, 'open': 1, 'pour': 2, 'close': 3, 'shake': 4,\n",
    "            'scoop': 5, 'stir': 6, 'put': 7, 'fold': 8, 'spread': 9, 'background': 10\n",
    "        }\n",
    "\n",
    "        # Transform for resizing and normalizing frames\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # Resize to ResNet input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "        ])\n",
    "\n",
    "        # Load data\n",
    "        self.frames = self._load_frames()\n",
    "        self.actions = self._load_actions()\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _load_frames(self):\n",
    "        \"\"\"Load all frames from the video frames directory.\"\"\"\n",
    "        frame_files = sorted(os.listdir(self.video_frames_path))  # Sort by frame order\n",
    "        frames = [\n",
    "            cv2.imread(os.path.join(self.video_frames_path, frame))\n",
    "            for frame in frame_files\n",
    "        ]\n",
    "        return frames\n",
    "\n",
    "    def _load_actions(self):\n",
    "        \"\"\"Load actions from the action .txt file.\"\"\"\n",
    "        with open(self.action_txt_path, \"r\") as f:\n",
    "            actions = [line.strip() for line in f.readlines()]\n",
    "        # Map actions to numerical values\n",
    "        return [self.mapping[action] for action in actions]\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        \"\"\"Parse the XML file and load annotations.\"\"\"\n",
    "        tree = ET.parse(self.xml_labels_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Dictionary to combine annotations by frame_id\n",
    "        combined_annotations = defaultdict(lambda: {\n",
    "            \"frame_id\": None,\n",
    "            \"width\": None,\n",
    "            \"height\": None,\n",
    "            \"boxes\": [],\n",
    "            \"keypoints\": []\n",
    "        })\n",
    "\n",
    "        for image in root.findall(\"image\"):\n",
    "            frame_id = image.get(\"id\")\n",
    "            width = int(image.get(\"width\"))\n",
    "            height = int(image.get(\"height\"))\n",
    "\n",
    "            # Initialize combined data for this frame_id\n",
    "            combined_annotations[frame_id][\"frame_id\"] = frame_id\n",
    "            combined_annotations[frame_id][\"width\"] = width\n",
    "            combined_annotations[frame_id][\"height\"] = height\n",
    "\n",
    "            # Extract bounding boxes\n",
    "            for box in image.findall(\"box\"):\n",
    "                xtl = float(box.get(\"xtl\"))\n",
    "                ytl = float(box.get(\"ytl\"))\n",
    "                xbr = float(box.get(\"xbr\"))\n",
    "                ybr = float(box.get(\"ybr\"))\n",
    "                label = box.get(\"label\")\n",
    "                hand_type = box.find(\"attribute\").text\n",
    "                combined_annotations[frame_id][\"boxes\"].append({\n",
    "                    \"xtl\": xtl,\n",
    "                    \"ytl\": ytl,\n",
    "                    \"xbr\": xbr,\n",
    "                    \"ybr\": ybr,\n",
    "                    \"label\": label,\n",
    "                    \"hand_type\": hand_type,\n",
    "                })\n",
    "\n",
    "            # Extract keypoints\n",
    "            for polyline in image.findall(\"polyline\"):\n",
    "                label = polyline.get(\"label\")\n",
    "                points = polyline.get(\"points\")\n",
    "                points = [\n",
    "                    tuple(map(float, point.split(\",\")))\n",
    "                    for point in points.split(\";\") if point.strip()\n",
    "                ]\n",
    "                combined_annotations[frame_id][\"keypoints\"].append({\n",
    "                    \"label\": label,\n",
    "                    \"points\": points,\n",
    "                })\n",
    "\n",
    "        # Return a list of combined annotations sorted by frame_id\n",
    "        return [combined_annotations[frame_id] for frame_id in sorted(combined_annotations)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of frames.\"\"\"\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return data for a specific frame.\"\"\"\n",
    "        frame = self.frames[idx]\n",
    "        action = self.actions[idx] if idx < len(self.actions) else self.mapping['background']\n",
    "        annotation = self.annotations[idx] if idx < len(self.annotations) else None\n",
    "\n",
    "        # Transform frame (resize and normalize)\n",
    "        frame = self.transform(frame)\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        coordinates = [\n",
    "            [box['xtl'], box['ytl'], box['xbr'], box['ybr']]\n",
    "            for box in annotation['boxes']\n",
    "        ]\n",
    "        coordinates_array = torch.tensor(coordinates, dtype=torch.float32)\n",
    "\n",
    "        # Separate keypoints by hand type\n",
    "        left_hand_keypoints = []\n",
    "        right_hand_keypoints = []\n",
    "\n",
    "        for kp in annotation['keypoints']:\n",
    "            if kp['label'] == 'thumb' or kp['label'].startswith('index') or kp['label'].startswith('middle') or kp['label'].startswith('ring') or kp['label'].startswith('pinkie'):\n",
    "                if 'left' in kp['label']:\n",
    "                    left_hand_keypoints.extend(kp['points'])\n",
    "                elif 'right' in kp['label']:\n",
    "                    right_hand_keypoints.extend(kp['points'])\n",
    "\n",
    "        # Convert to NumPy arrays and pad if necessary\n",
    "        left_hand_array = np.array(left_hand_keypoints) if left_hand_keypoints else np.zeros((21, 2))\n",
    "        right_hand_array = np.array(right_hand_keypoints) if right_hand_keypoints else np.zeros((21, 2))\n",
    "\n",
    "        # Ensure both arrays are of size (21, 2)\n",
    "        if left_hand_array.shape[0] != 21:\n",
    "            left_hand_array = np.vstack([left_hand_array, np.zeros((21 - left_hand_array.shape[0], 2))])\n",
    "        if right_hand_array.shape[0] != 21:\n",
    "            right_hand_array = np.vstack([right_hand_array, np.zeros((21 - right_hand_array.shape[0], 2))])\n",
    "\n",
    "        # Combine left and right hand keypoints into a single array of size (42, 2)\n",
    "        keypoints_array = np.vstack([left_hand_array, right_hand_array])\n",
    "        keypoints_array = torch.tensor(keypoints_array, dtype=torch.float32)\n",
    "\n",
    "        # Pad bounding boxes if one is missing\n",
    "        if len(annotation['boxes']) < 2:\n",
    "            if any(box['hand_type'] == 'left' for box in annotation['boxes']):\n",
    "                # Pad a zero bounding box for the right hand\n",
    "                coordinates_array = torch.cat([coordinates_array, torch.zeros((1, 4))])\n",
    "            elif any(box['hand_type'] == 'right' for box in annotation['boxes']):\n",
    "                # Pad a zero bounding box for the left hand\n",
    "                coordinates_array = torch.cat([torch.zeros((1, 4)), coordinates_array])\n",
    "\n",
    "        return frame, coordinates_array, keypoints_array, action, annotation['frame_id']\n",
    "\n",
    "# Example Usage\n",
    "video_frames_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\gtea_png\\gtea_png\\png\\S1_Cheese_C1\"\n",
    "action_txt_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\actions-20241207T081235Z-001\\actions\\S1_Cheese_C1.txt\"\n",
    "xml_labels_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\xml_labels\\S1_Cheese_C1.xml\"\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 5\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class SimpleVideoDataset(Dataset):\n",
    "    def __init__(self, video_frames_path, action_txt_path, xml_labels_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_frames_path (str): Path to the folder containing video frames.\n",
    "            action_txt_path (str): Path to the .txt file containing action labels.\n",
    "            xml_labels_path (str): Path to the .xml file containing annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample (frame).\n",
    "        \"\"\"\n",
    "        self.video_frames_path = video_frames_path\n",
    "        self.action_txt_path = action_txt_path\n",
    "        self.xml_labels_path = xml_labels_path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Action mapping\n",
    "        self.mapping = {\n",
    "            'take': 0, 'open': 1, 'pour': 2, 'close': 3, 'shake': 4,\n",
    "            'scoop': 5, 'stir': 6, 'put': 7, 'fold': 8, 'spread': 9, 'background': 10\n",
    "        }\n",
    "\n",
    "        # Load data\n",
    "        self.frames = self._load_frames()\n",
    "        self.actions = self._load_actions()\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _load_frames(self):\n",
    "        \"\"\"Load all frames from the video frames directory.\"\"\"\n",
    "        frame_files = sorted(os.listdir(self.video_frames_path))  # Sort by frame order\n",
    "        frames = [\n",
    "            cv2.imread(os.path.join(self.video_frames_path, frame))\n",
    "            for frame in frame_files\n",
    "        ]\n",
    "        return frames\n",
    "\n",
    "    def _load_actions(self):\n",
    "        \"\"\"Load actions from the action .txt file.\"\"\"\n",
    "        with open(self.action_txt_path, \"r\") as f:\n",
    "            actions = [line.strip() for line in f.readlines()]\n",
    "        # Map actions to numerical values\n",
    "        return [self.mapping[action] for action in actions]\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        \"\"\"Parse the XML file and load annotations.\"\"\"\n",
    "        tree = ET.parse(self.xml_labels_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Dictionary to combine annotations by frame_id\n",
    "        combined_annotations = defaultdict(lambda: {\n",
    "            \"frame_id\": None,\n",
    "            \"width\": None,\n",
    "            \"height\": None,\n",
    "            \"boxes\": [],\n",
    "            \"keypoints\": []\n",
    "        })\n",
    "\n",
    "        for image in root.findall(\"image\"):\n",
    "            frame_id = image.get(\"id\")\n",
    "            width = int(image.get(\"width\"))\n",
    "            height = int(image.get(\"height\"))\n",
    "\n",
    "            # Initialize combined data for this frame_id\n",
    "            combined_annotations[frame_id][\"frame_id\"] = frame_id\n",
    "            combined_annotations[frame_id][\"width\"] = width\n",
    "            combined_annotations[frame_id][\"height\"] = height\n",
    "\n",
    "            # Extract bounding boxes\n",
    "            for box in image.findall(\"box\"):\n",
    "                xtl = float(box.get(\"xtl\"))\n",
    "                ytl = float(box.get(\"ytl\"))\n",
    "                xbr = float(box.get(\"xbr\"))\n",
    "                ybr = float(box.get(\"ybr\"))\n",
    "                label = box.get(\"label\")\n",
    "                hand_type = box.find(\"attribute\").text\n",
    "                combined_annotations[frame_id][\"boxes\"].append({\n",
    "                    \"xtl\": xtl,\n",
    "                    \"ytl\": ytl,\n",
    "                    \"xbr\": xbr,\n",
    "                    \"ybr\": ybr,\n",
    "                    \"label\": label,\n",
    "                    \"hand_type\": hand_type,\n",
    "                })\n",
    "\n",
    "            # Extract keypoints\n",
    "            for polyline in image.findall(\"polyline\"):\n",
    "                label = polyline.get(\"label\")\n",
    "                points = polyline.get(\"points\")\n",
    "                points = [\n",
    "                    tuple(map(float, point.split(\",\")))\n",
    "                    for point in points.split(\";\") if point.strip()\n",
    "                ]\n",
    "                combined_annotations[frame_id][\"keypoints\"].append({\n",
    "                    \"label\": label,\n",
    "                    \"points\": points,\n",
    "                })\n",
    "\n",
    "        # Return a list of combined annotations sorted by frame_id\n",
    "        return [combined_annotations[frame_id] for frame_id in sorted(combined_annotations)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of frames.\"\"\"\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return data for a specific frame.\"\"\"\n",
    "        frame = self.frames[idx]\n",
    "        action = self.actions[idx] if idx < len(self.actions) else self.mapping['background']\n",
    "        annotation = self.annotations[idx] if idx < len(self.annotations) else None\n",
    "\n",
    "        # Apply transforms to frame\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        coordinates = [\n",
    "            [box['xtl'], box['ytl'], box['xbr'], box['ybr']]\n",
    "            for box in annotation['boxes']\n",
    "        ]\n",
    "        coordinates_array = torch.tensor(coordinates, dtype=torch.float32)\n",
    "\n",
    "        # Separate keypoints by hand type\n",
    "        left_hand_keypoints = []\n",
    "        right_hand_keypoints = []\n",
    "\n",
    "        for kp in annotation['keypoints']:\n",
    "            if kp['label'] == 'thumb' or kp['label'].startswith('index') or kp['label'].startswith('middle') or kp['label'].startswith('ring') or kp['label'].startswith('pinkie'):\n",
    "                if 'left' in kp['label']:\n",
    "                    left_hand_keypoints.extend(kp['points'])\n",
    "                elif 'right' in kp['label']:\n",
    "                    right_hand_keypoints.extend(kp['points'])\n",
    "\n",
    "        # Convert to NumPy arrays and pad if necessary\n",
    "        left_hand_array = np.array(left_hand_keypoints) if left_hand_keypoints else np.zeros((21, 2))\n",
    "        right_hand_array = np.array(right_hand_keypoints) if right_hand_keypoints else np.zeros((21, 2))\n",
    "\n",
    "        # Ensure both arrays are of size (21, 2)\n",
    "        if left_hand_array.shape[0] != 21:\n",
    "            left_hand_array = np.vstack([left_hand_array, np.zeros((21 - left_hand_array.shape[0], 2))])\n",
    "        if right_hand_array.shape[0] != 21:\n",
    "            right_hand_array = np.vstack([right_hand_array, np.zeros((21 - right_hand_array.shape[0], 2))])\n",
    "\n",
    "        # Combine left and right hand keypoints into a single array of size (42, 2)\n",
    "        keypoints_array = np.vstack([left_hand_array, right_hand_array])\n",
    "        keypoints_array = torch.tensor(keypoints_array, dtype=torch.float32)\n",
    "\n",
    "        # Pad bounding boxes if one is missing\n",
    "        if len(annotation['boxes']) < 2:\n",
    "            if any(box['hand_type'] == 'left' for box in annotation['boxes']):\n",
    "                # Pad a zero bounding box for the right hand\n",
    "                coordinates_array = torch.cat([coordinates_array, torch.zeros((1, 4))])\n",
    "            elif any(box['hand_type'] == 'right' for box in annotation['boxes']):\n",
    "                # Pad a zero bounding box for the left hand\n",
    "                coordinates_array = torch.cat([torch.zeros((1, 4)), coordinates_array])\n",
    "\n",
    "        return frame, coordinates_array, keypoints_array, action, annotation['frame_id']\n",
    "\n",
    "\n",
    "# Define Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((404, 720)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "dataset = SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Shape: torch.Size([1, 3, 404, 720])\n",
      "Coordinates Shape: torch.Size([1, 2, 4])\n",
      "Keypoints Shape: torch.Size([1, 42, 2])\n",
      "Actions: tensor([7])\n",
      "Frame IDs: ('frame_000291',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    frames, coordinates, keypoints, actions, frame_ids = batch\n",
    "    print(\"Frames Shape:\", frames.shape)\n",
    "    print(\"Coordinates Shape:\", coordinates.shape)\n",
    "    print(\"Keypoints Shape:\", keypoints.shape)\n",
    "    print(\"Actions:\", actions)\n",
    "    print(\"Frame IDs:\", frame_ids)\n",
    "    break\n",
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: torch.Size([5, 3, 404, 720])\n",
      "Coordinates: torch.Size([5, 2, 4])\n",
      "Keypoints: torch.Size([5, 42, 2])\n",
      "Actions: tensor([ 0,  7,  7, 10, 10])\n",
      "Frame IDs: ('frame_000164', 'frame_000749', 'frame_000502', 'frame_000083', 'frame_000891')\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    frames, coordinates, keypoints, actions, frame_ids = batch\n",
    "    print(\"Frames:\", frames.shape)\n",
    "    print(\"Coordinates:\", coordinates.shape)\n",
    "    print(\"Keypoints:\", keypoints.shape)\n",
    "    print(\"Actions:\", actions)\n",
    "    print(\"Frame IDs:\", frame_ids)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping= {0: 'take',\n",
    "1: 'open',\n",
    "2: 'pour',\n",
    "3: 'close',\n",
    "4: 'shake',\n",
    "5: 'scoop',\n",
    "6: 'stir',\n",
    "7: 'put',\n",
    "8: 'fold',\n",
    "9: 'spread',\n",
    "10: 'background'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'background'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m xml_labels_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mabdul\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvisionrd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAI-Hackathon24\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mxml_labels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS1_Cheese_C1.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscoop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m6\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m7\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m8\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m9\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m--> 152\u001b[0m dataset \u001b[38;5;241m=\u001b[39m SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path, mapping)\n\u001b[0;32m    153\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Initialize model, criterion, and optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 21\u001b[0m, in \u001b[0;36mSimpleVideoDataset.__init__\u001b[1;34m(self, video_frames_path, action_txt_path, xml_labels_path, mapping)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping \u001b[38;5;241m=\u001b[39m mapping\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_frames()\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_actions()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_annotations()\n",
      "Cell \u001b[1;32mIn[57], line 35\u001b[0m, in \u001b[0;36mSimpleVideoDataset._load_actions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_txt_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     34\u001b[0m     actions \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping[action] \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m actions]\n",
      "Cell \u001b[1;32mIn[57], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_txt_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     34\u001b[0m     actions \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping[action] \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m actions]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'background'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=11, image_height=404, image_width=720):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "\n",
    "        # Transform bounding boxes and keypoints\n",
    "        self.bbox_fc = nn.Linear(4 * 2, image_width)  # Assume 2 boxes\n",
    "        self.kp_fc = nn.Linear(42 * 2, image_width)   # 42 keypoints\n",
    "\n",
    "        # Combine features into 2D\n",
    "        self.combine_fc = nn.Linear(image_width * 2, image_height * image_width)\n",
    "\n",
    "        # ResNet backbone\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Adjust input channels to 4\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, frame, boxes, keypoints):\n",
    "        # Transform bounding boxes and keypoints\n",
    "        bbox_feat = self.bbox_fc(boxes).unsqueeze(1)\n",
    "        kp_feat = self.kp_fc(keypoints).unsqueeze(1)\n",
    "\n",
    "        # Combine and reshape into 2D\n",
    "        combined_feat = torch.cat([bbox_feat, kp_feat], dim=1)\n",
    "        combined_feat = self.combine_fc(combined_feat).view(-1, 1, self.image_height, self.image_width)\n",
    "\n",
    "        # Concatenate with image\n",
    "        frame = torch.cat([frame, combined_feat], dim=1)\n",
    "\n",
    "        # ResNet classification\n",
    "        return self.resnet(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 720, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.,   0., 349., 198., 418., 297.],\n",
       "        [  0.,   0.,   0.,   0., 379.,  97., 570., 187.],\n",
       "        [148., 191., 304., 325.,   0.,   0.,   0.,   0.],\n",
       "        [204., 311., 300., 384., 340., 260., 459., 344.],\n",
       "        [239., 200., 334., 315., 358., 189., 453., 317.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates.view(batch_size,  8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.,   0.],\n",
       "         [349., 198., 418., 297.]],\n",
       "\n",
       "        [[  0.,   0.,   0.,   0.],\n",
       "         [379.,  97., 570., 187.]],\n",
       "\n",
       "        [[148., 191., 304., 325.],\n",
       "         [  0.,   0.,   0.,   0.]],\n",
       "\n",
       "        [[204., 311., 300., 384.],\n",
       "         [340., 260., 459., 344.]],\n",
       "\n",
       "        [[239., 200., 334., 315.],\n",
       "         [358., 189., 453., 317.]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3539022207260132\n",
      "Epoch 2, Loss: 1.2201240062713623\n",
      "Epoch 3, Loss: 1.2203675508499146\n",
      "Epoch 4, Loss: 0.8758986592292786\n",
      "Epoch 5, Loss: 1.2039836645126343\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Dataset Class\n",
    "class SimpleVideoDataset(Dataset):\n",
    "    def __init__(self, video_frames_path, action_txt_path, xml_labels_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_frames_path (str): Path to the folder containing video frames.\n",
    "            action_txt_path (str): Path to the .txt file containing action labels.\n",
    "            xml_labels_path (str): Path to the .xml file containing annotations.\n",
    "        \"\"\"\n",
    "        self.video_frames_path = video_frames_path\n",
    "        self.action_txt_path = action_txt_path\n",
    "        self.xml_labels_path = xml_labels_path\n",
    "\n",
    "        # Action mapping\n",
    "        self.mapping = {\n",
    "            'take': 0, 'open': 1, 'pour': 2, 'close': 3, 'shake': 4,\n",
    "            'scoop': 5, 'stir': 6, 'put': 7, 'fold': 8, 'spread': 9, 'background': 10\n",
    "        }\n",
    "\n",
    "        # Transform for resizing and normalizing frames\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # Resize to ResNet input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "        ])\n",
    "\n",
    "        # Load data\n",
    "        self.frames = self._load_frames()\n",
    "        self.actions = self._load_actions()\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _load_frames(self):\n",
    "        \"\"\"Load all frames from the video frames directory.\"\"\"\n",
    "        frame_files = sorted(os.listdir(self.video_frames_path))  # Sort by frame order\n",
    "        frames = [\n",
    "            cv2.imread(os.path.join(self.video_frames_path, frame))\n",
    "            for frame in frame_files\n",
    "        ]\n",
    "        return frames\n",
    "\n",
    "    def _load_actions(self):\n",
    "        \"\"\"Load actions from the action .txt file.\"\"\"\n",
    "        with open(self.action_txt_path, \"r\") as f:\n",
    "            actions = [line.strip() for line in f.readlines()]\n",
    "        # Map actions to numerical values\n",
    "        return [self.mapping[action] for action in actions]\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        \"\"\"Parse the XML file and load annotations.\"\"\"\n",
    "        tree = ET.parse(self.xml_labels_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Dictionary to combine annotations by frame_id\n",
    "        combined_annotations = defaultdict(lambda: {\n",
    "            \"frame_id\": None,\n",
    "            \"width\": None,\n",
    "            \"height\": None,\n",
    "            \"boxes\": [],\n",
    "            \"keypoints\": []\n",
    "        })\n",
    "\n",
    "        for image in root.findall(\"image\"):\n",
    "            frame_id = image.get(\"id\")\n",
    "            width = int(image.get(\"width\"))\n",
    "            height = int(image.get(\"height\"))\n",
    "\n",
    "            # Initialize combined data for this frame_id\n",
    "            combined_annotations[frame_id][\"frame_id\"] = frame_id\n",
    "            combined_annotations[frame_id][\"width\"] = width\n",
    "            combined_annotations[frame_id][\"height\"] = height\n",
    "\n",
    "            # Extract bounding boxes\n",
    "            for box in image.findall(\"box\"):\n",
    "                xtl = float(box.get(\"xtl\"))\n",
    "                ytl = float(box.get(\"ytl\"))\n",
    "                xbr = float(box.get(\"xbr\"))\n",
    "                ybr = float(box.get(\"ybr\"))\n",
    "                label = box.get(\"label\")\n",
    "                hand_type = box.find(\"attribute\").text\n",
    "                combined_annotations[frame_id][\"boxes\"].append({\n",
    "                    \"xtl\": xtl,\n",
    "                    \"ytl\": ytl,\n",
    "                    \"xbr\": xbr,\n",
    "                    \"ybr\": ybr,\n",
    "                    \"label\": label,\n",
    "                    \"hand_type\": hand_type,\n",
    "                })\n",
    "\n",
    "            # Extract keypoints\n",
    "            for polyline in image.findall(\"polyline\"):\n",
    "                label = polyline.get(\"label\")\n",
    "                points = polyline.get(\"points\")\n",
    "                points = [\n",
    "                    tuple(map(float, point.split(\",\")))\n",
    "                    for point in points.split(\";\") if point.strip()\n",
    "                ]\n",
    "                combined_annotations[frame_id][\"keypoints\"].append({\n",
    "                    \"label\": label,\n",
    "                    \"points\": points,\n",
    "                })\n",
    "\n",
    "        # Return a list of combined annotations sorted by frame_id\n",
    "        return [combined_annotations[frame_id] for frame_id in sorted(combined_annotations)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of frames.\"\"\"\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return data for a specific frame.\"\"\"\n",
    "        frame = self.frames[idx]\n",
    "        action = self.actions[idx] if idx < len(self.actions) else self.mapping['background']\n",
    "        annotation = self.annotations[idx] if idx < len(self.annotations) else None\n",
    "\n",
    "        # Transform frame (resize and normalize)\n",
    "        frame = self.transform(frame)\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        coordinates = [\n",
    "            [box['xtl'], box['ytl'], box['xbr'], box['ybr']]\n",
    "            for box in annotation['boxes']\n",
    "        ]\n",
    "        coordinates_array = torch.tensor(coordinates, dtype=torch.float32)\n",
    "\n",
    "        # Separate keypoints by hand type\n",
    "        left_hand_keypoints = []\n",
    "        right_hand_keypoints = []\n",
    "\n",
    "        for kp in annotation['keypoints']:\n",
    "            if kp['label'] == 'thumb' or kp['label'].startswith('index') or kp['label'].startswith('middle') or kp['label'].startswith('ring') or kp['label'].startswith('pinkie'):\n",
    "                if 'left' in kp['label']:\n",
    "                    left_hand_keypoints.extend(kp['points'])\n",
    "                elif 'right' in kp['label']:\n",
    "                    right_hand_keypoints.extend(kp['points'])\n",
    "\n",
    "        # Convert to NumPy arrays and pad if necessary\n",
    "        left_hand_array = np.array(left_hand_keypoints) if left_hand_keypoints else np.zeros((21, 2))\n",
    "        right_hand_array = np.array(right_hand_keypoints) if right_hand_keypoints else np.zeros((21, 2))\n",
    "\n",
    "        # Ensure both arrays are of size (21, 2)\n",
    "        if left_hand_array.shape[0] != 21:\n",
    "            left_hand_array = np.vstack([left_hand_array, np.zeros((21 - left_hand_array.shape[0], 2))])\n",
    "        if right_hand_array.shape[0] != 21:\n",
    "            right_hand_array = np.vstack([right_hand_array, np.zeros((21 - right_hand_array.shape[0], 2))])\n",
    "\n",
    "        # Combine left and right hand keypoints into a single array of size (42, 2)\n",
    "        keypoints_array = np.vstack([left_hand_array, right_hand_array])\n",
    "        keypoints_array = torch.tensor(keypoints_array, dtype=torch.float32)\n",
    "\n",
    "        # Pad bounding boxes if one is missing\n",
    "        if len(annotation['boxes']) < 2:\n",
    "            if any(box['hand_type'] == 'left' for box in annotation['boxes']):\n",
    "                # Pad a zero bounding box for the right hand\n",
    "                coordinates_array = torch.cat([coordinates_array, torch.zeros((1, 4))])\n",
    "            elif any(box['hand_type'] == 'right' for box in annotation['boxes']):\n",
    "                # Pad a zero bounding box for the left hand\n",
    "                coordinates_array = torch.cat([torch.zeros((1, 4)), coordinates_array])\n",
    "\n",
    "        return frame, coordinates_array, keypoints_array, action, annotation['frame_id']\n",
    "\n",
    "\n",
    "\n",
    "video_frames_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\gtea_png\\gtea_png\\png\\S1_Cheese_C1\"\n",
    "action_txt_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\actions-20241207T081235Z-001\\actions\\S1_Cheese_C1.txt\"\n",
    "xml_labels_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\xml_labels\\S1_Cheese_C1.xml\"\n",
    "\n",
    "\n",
    "dataset = SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Model with Custom Depth Handling\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=11, image_size=224):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Transform bounding boxes and keypoints\n",
    "        self.bbox_fc = nn.Linear(4 * 2, image_size)  # Assume 2 boxes, adjust if needed\n",
    "        self.kp_fc = nn.Linear(42 * 2, image_size)  # 42 keypoints * 2 coordinates (x, y)\n",
    "\n",
    "        # Combine transformed features into a depth map\n",
    "        self.combine_fc = nn.Linear(image_size * 2, image_size * image_size)\n",
    "\n",
    "        # Pretrained ResNet modified for 4-channel input\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, frame, boxes, keypoints):\n",
    "        # Flatten and transform bounding boxes and keypoints\n",
    "        boxes_flat = boxes.view(boxes.size(0), -1)  # Flatten boxes\n",
    "        keypoints_flat = keypoints.view(keypoints.size(0), -1)  # Flatten keypoints\n",
    "\n",
    "        bbox_feat = self.bbox_fc(boxes_flat)  # Transform bounding boxes\n",
    "        kp_feat = self.kp_fc(keypoints_flat)  # Transform keypoints\n",
    "\n",
    "        # Combine transformed features and reshape to (B, 1, H, W)\n",
    "        combined_feat = torch.cat([bbox_feat, kp_feat], dim=1)\n",
    "        depth_map = self.combine_fc(combined_feat).view(-1, 1, self.image_size, self.image_size)\n",
    "\n",
    "        # Concatenate depth map with the image\n",
    "        frame = torch.cat([frame, depth_map], dim=1)\n",
    "\n",
    "        # Forward pass through ResNet\n",
    "        return self.resnet(frame)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop with Updated Data Handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = CustomNetwork().to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        frame, boxes, keypoints, action, _ = batch\n",
    "\n",
    "        # Send data to GPU if available\n",
    "        frame, boxes, keypoints, action = frame.to(device), boxes.to(device), keypoints.to(device), action.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(frame, boxes, keypoints)\n",
    "\n",
    "        # Compute loss and update weights\n",
    "        loss = criterion(output, action)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVideoDataset(Dataset):\n",
    "    def __init__(self, video_frames_path, action_txt_path, xml_labels_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_frames_path (str): Path to the folder containing video frames.\n",
    "            action_txt_path (str): Path to the .txt file containing action labels.\n",
    "            xml_labels_path (str): Path to the .xml file containing annotations.\n",
    "        \"\"\"\n",
    "        self.video_frames_path = video_frames_path\n",
    "        self.action_txt_path = action_txt_path\n",
    "        self.xml_labels_path = xml_labels_path\n",
    "\n",
    "        # Action mapping\n",
    "        self.mapping = {\n",
    "            'take': 0, 'open': 1, 'pour': 2, 'close': 3, 'shake': 4,\n",
    "            'scoop': 5, 'stir': 6, 'put': 7, 'fold': 8, 'spread': 9, 'background': 10\n",
    "        }\n",
    "\n",
    "        # Transform for resizing and normalizing frames\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # Resize to ResNet input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "        ])\n",
    "\n",
    "        # Load data\n",
    "        self.frames = self._load_frames()\n",
    "        self.actions = self._load_actions()\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _load_frames(self):\n",
    "        \"\"\"Load all frames from the video frames directory.\"\"\"\n",
    "        frame_files = sorted(os.listdir(self.video_frames_path))  # Sort by frame order\n",
    "        frames = [\n",
    "            cv2.imread(os.path.join(self.video_frames_path, frame))\n",
    "            for frame in frame_files\n",
    "        ]\n",
    "        return frames\n",
    "\n",
    "    def _load_actions(self):\n",
    "        \"\"\"Load actions from the action .txt file.\"\"\"\n",
    "        with open(self.action_txt_path, \"r\") as f:\n",
    "            actions = [line.strip() for line in f.readlines()]\n",
    "        # Map actions to numerical values\n",
    "        return [self.mapping[action] for action in actions]\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        \"\"\"Parse the XML file and load annotations.\"\"\"\n",
    "        tree = ET.parse(self.xml_labels_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        annotations = defaultdict(lambda: {\"boxes\": [], \"keypoints\": []})\n",
    "\n",
    "        for image in root.findall(\"image\"):\n",
    "            # Extract the numeric part of the frame ID\n",
    "            frame_id_str = image.get(\"id\")\n",
    "            frame_id = int(frame_id_str.split(\"_\")[-1])  # Extract the numeric ID\n",
    "\n",
    "            # Extract bounding boxes\n",
    "            for box in image.findall(\"box\"):\n",
    "                xtl = float(box.get(\"xtl\"))\n",
    "                ytl = float(box.get(\"ytl\"))\n",
    "                xbr = float(box.get(\"xbr\"))\n",
    "                ybr = float(box.get(\"ybr\"))\n",
    "                annotations[frame_id][\"boxes\"].append([xtl, ytl, xbr, ybr])\n",
    "\n",
    "            # Extract keypoints\n",
    "            for polyline in image.findall(\"polyline\"):\n",
    "                points = polyline.get(\"points\")\n",
    "                points = [\n",
    "                    tuple(map(float, point.split(\",\")))\n",
    "                    for point in points.split(\";\") if point.strip()\n",
    "                ]\n",
    "                annotations[frame_id][\"keypoints\"].append(points)\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of frames.\"\"\"\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Collect 5-frame sequence (including padded start/end)\n",
    "        sequence_frames = []\n",
    "        sequence_boxes = []\n",
    "        sequence_keypoints = []\n",
    "\n",
    "        for offset in range(-2, 3):\n",
    "            frame_idx = max(0, min(len(self.frames) - 1, idx + offset))\n",
    "            frame = self.frames[frame_idx]\n",
    "            annotation = self.annotations[frame_idx]\n",
    "\n",
    "            # Transform frame\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "            # Extract bounding boxes and keypoints\n",
    "            boxes = torch.tensor(\n",
    "                [[box[0], box[1], box[2], box[3]] for box in annotation[\"boxes\"]]\n",
    "            ).float()\n",
    "            if len(boxes) < 2:\n",
    "                boxes = torch.cat([boxes, torch.zeros((2 - len(boxes), 4))])\n",
    "\n",
    "            keypoints = torch.tensor(\n",
    "                [(kp[0], kp[1]) for kps in annotation[\"keypoints\"] for kp in kps]\n",
    "            ).float()\n",
    "            if len(keypoints) < 42:\n",
    "                keypoints = torch.cat([keypoints, torch.zeros((42 - len(keypoints), 2))])\n",
    "\n",
    "            sequence_frames.append(frame)\n",
    "            sequence_boxes.append(boxes)\n",
    "            sequence_keypoints.append(keypoints)\n",
    "\n",
    "        return (\n",
    "            torch.stack(sequence_frames),\n",
    "            torch.stack(sequence_boxes),\n",
    "            torch.stack(sequence_keypoints),\n",
    "            self.actions[idx],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (frames, boxes, keypoints, actions) in enumerate(dataloader):\n",
    "#     # Debugging: Check the shape of the frames tensor\n",
    "#     print(f\"Original Frames Shape: {frames.shape}\")  # Should be (B, T, C, H, W)\n",
    "    \n",
    "#     # Permute the tensor to match VideoMAE's expected shape\n",
    "#     frames = frames.permute(0, 2, 1, 3, 4)  # From (B, T, C, H, W) to (B, C, T, H, W)\n",
    "    \n",
    "#     # Debugging: Check the shape after permutation\n",
    "#     print(f\"Permuted Frames Shape: {frames.shape}\")  # Should be (B, C, T, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_dataset_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoMAEForVideoClassification, VideoMAEConfig\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myour_dataset_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleVideoDataset  \u001b[38;5;66;03m# Replace with actual import\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Paths to your data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m video_frames_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mabdul\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvisionrd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAI-Hackathon24\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgtea_png\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgtea_png\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS1_Cheese_C1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'your_dataset_module'"
     ]
    }
   ],
   "source": [
    "video_frames_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\gtea_png\\gtea_png\\png\\S1_Cheese_C1\"\n",
    "action_txt_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\actions-20241207T081235Z-001\\actions\\S1_Cheese_C1.txt\"\n",
    "xml_labels_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\xml_labels\\S1_Cheese_C1.xml\"\n",
    "\n",
    "dataset = SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# # Test\n",
    "# for batch_idx, (frames, boxes, keypoints, actions) in enumerate(dataloader):\n",
    "#     print(f\"Batch {batch_idx+1}\")\n",
    "#     print(f\"Frames Shape: {frames.shape}\")  # Shape: (batch_size, 5, 3, 224, 224)\n",
    "#     print(f\"Boxes Shape: {boxes.shape}\")    # Shape: (batch_size, 5, 2, 4)\n",
    "#     print(f\"Keypoints Shape: {keypoints.shape}\")  # Shape: (batch_size, 5, 42, 2)\n",
    "#     print(f\"Actions Shape: {actions.shape}\")  # Shape: (batch_size, 5)\n",
    "#     break\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEConfig\n",
    "# Define configuration\n",
    "config = VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "config.num_frames = 5\n",
    "config.image_size = 224\n",
    "config.output_attentions = True\n",
    "\n",
    "# Load model with custom configuration\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\", config=config).to(device)\n",
    "\n",
    "# Iterate over dataloader\n",
    "for batch_idx, (frames, _, _, _) in enumerate(dataloader):\n",
    "    # Normalize frames and permute dimensions\n",
    "    frames = frames.permute(0, 2, 1, 3, 4).to(device)  # (B, C, T, H, W)\n",
    "    frames = frames / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    import pdb\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Attention Layer 1: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 2: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 3: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 4: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 5: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 6: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 7: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 8: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 9: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 10: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 11: torch.Size([2, 12, 392, 392])\n",
      "Attention Layer 12: torch.Size([2, 12, 392, 392])\n",
      "> \u001b[1;32mc:\\users\\abdul\\appdata\\local\\temp\\ipykernel_25836\\2842341824.py\u001b[0m(46)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEConfig\n",
    "\n",
    "# Paths to your data\n",
    "video_frames_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\gtea_png\\gtea_png\\png\\S1_Cheese_C1\"\n",
    "action_txt_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\actions-20241207T081235Z-001\\actions\\S1_Cheese_C1.txt\"\n",
    "xml_labels_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\xml_labels\\S1_Cheese_C1.xml\"\n",
    "\n",
    "# Define dataset and dataloader\n",
    "dataset = SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define VideoMAE configuration\n",
    "config = VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "config.num_frames = 5\n",
    "config.image_size = 224\n",
    "config.output_attentions = True\n",
    "\n",
    "# Load model with custom configuration\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\", config=config).to(device)\n",
    "\n",
    "# Iterate over dataloader\n",
    "for batch_idx, (frames, _, _, _) in enumerate(dataloader):\n",
    "    # Normalize frames and permute dimensions\n",
    "    frames = frames / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Forward pass through the model to get attention scores\n",
    "    outputs = model(frames.to(device))\n",
    "    attention_scores = outputs.attentions\n",
    "\n",
    "    # Print attention scores\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    for i, attention in enumerate(attention_scores):\n",
    "        print(f\"Attention Layer {i + 1}: {attention.shape}\")\n",
    "\n",
    "    # Debugging if needed\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "\n",
    "    # Process only the first batch for testing\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[338., 209., 408., 287.],\n",
      "          [  0.,   0.,   0.,   0.]],\n",
      "\n",
      "         [[333., 209., 410., 286.],\n",
      "          [  0.,   0.,   0.,   0.]],\n",
      "\n",
      "         [[337., 204., 405., 273.],\n",
      "          [  0.,   0.,   0.,   0.]],\n",
      "\n",
      "         [[339., 202., 409., 282.],\n",
      "          [  0.,   0.,   0.,   0.]],\n",
      "\n",
      "         [[341., 199., 410., 289.],\n",
      "          [  0.,   0.,   0.,   0.]]]]) tensor([[[[402., 279.],\n",
      "          [375., 281.],\n",
      "          [358., 268.],\n",
      "          [349., 253.],\n",
      "          [346., 244.],\n",
      "          [361., 246.],\n",
      "          [344., 227.],\n",
      "          [346., 234.],\n",
      "          [352., 242.],\n",
      "          [375., 236.],\n",
      "          [358., 220.],\n",
      "          [358., 228.],\n",
      "          [364., 235.],\n",
      "          [388., 231.],\n",
      "          [373., 215.],\n",
      "          [371., 222.],\n",
      "          [375., 229.],\n",
      "          [399., 230.],\n",
      "          [388., 217.],\n",
      "          [385., 221.],\n",
      "          [388., 227.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.]],\n",
      "\n",
      "         [[404., 279.],\n",
      "          [377., 280.],\n",
      "          [359., 266.],\n",
      "          [347., 251.],\n",
      "          [339., 242.],\n",
      "          [363., 246.],\n",
      "          [345., 224.],\n",
      "          [345., 230.],\n",
      "          [351., 238.],\n",
      "          [377., 237.],\n",
      "          [359., 219.],\n",
      "          [358., 227.],\n",
      "          [363., 234.],\n",
      "          [389., 231.],\n",
      "          [373., 215.],\n",
      "          [371., 223.],\n",
      "          [375., 230.],\n",
      "          [400., 229.],\n",
      "          [388., 217.],\n",
      "          [384., 222.],\n",
      "          [387., 228.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.]],\n",
      "\n",
      "         [[399., 261.],\n",
      "          [378., 267.],\n",
      "          [359., 256.],\n",
      "          [349., 241.],\n",
      "          [343., 229.],\n",
      "          [366., 242.],\n",
      "          [349., 223.],\n",
      "          [348., 225.],\n",
      "          [352., 230.],\n",
      "          [378., 233.],\n",
      "          [362., 213.],\n",
      "          [360., 218.],\n",
      "          [364., 225.],\n",
      "          [389., 228.],\n",
      "          [376., 210.],\n",
      "          [374., 216.],\n",
      "          [379., 222.],\n",
      "          [399., 226.],\n",
      "          [389., 213.],\n",
      "          [388., 217.],\n",
      "          [392., 224.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.]],\n",
      "\n",
      "         [[403., 276.],\n",
      "          [373., 274.],\n",
      "          [354., 257.],\n",
      "          [346., 238.],\n",
      "          [345., 226.],\n",
      "          [363., 237.],\n",
      "          [347., 211.],\n",
      "          [346., 220.],\n",
      "          [350., 230.],\n",
      "          [379., 230.],\n",
      "          [360., 208.],\n",
      "          [356., 217.],\n",
      "          [360., 225.],\n",
      "          [392., 227.],\n",
      "          [374., 209.],\n",
      "          [370., 216.],\n",
      "          [374., 224.],\n",
      "          [401., 227.],\n",
      "          [387., 212.],\n",
      "          [382., 216.],\n",
      "          [384., 223.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.]],\n",
      "\n",
      "         [[403., 283.],\n",
      "          [373., 280.],\n",
      "          [354., 259.],\n",
      "          [347., 238.],\n",
      "          [348., 224.],\n",
      "          [373., 233.],\n",
      "          [356., 206.],\n",
      "          [353., 218.],\n",
      "          [356., 230.],\n",
      "          [387., 227.],\n",
      "          [370., 205.],\n",
      "          [364., 210.],\n",
      "          [364., 214.],\n",
      "          [398., 228.],\n",
      "          [380., 216.],\n",
      "          [374., 224.],\n",
      "          [375., 231.],\n",
      "          [404., 232.],\n",
      "          [388., 223.],\n",
      "          [383., 229.],\n",
      "          [385., 235.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.],\n",
      "          [  0.,   0.]]]])\n"
     ]
    }
   ],
   "source": [
    "dataset = SimpleVideoDataset(video_frames_path, action_txt_path, xml_labels_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch_idx, (_, boxes, keypoints, actions) in enumerate(dataloader):\n",
    "    frames = frames.to(device)\n",
    "    print(boxes, keypoints)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 42, 2])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[402., 279.],\n",
       "        [375., 281.],\n",
       "        [358., 268.],\n",
       "        [349., 253.],\n",
       "        [346., 244.],\n",
       "        [361., 246.],\n",
       "        [344., 227.],\n",
       "        [346., 234.],\n",
       "        [352., 242.],\n",
       "        [375., 236.],\n",
       "        [358., 220.],\n",
       "        [358., 228.],\n",
       "        [364., 235.],\n",
       "        [388., 231.],\n",
       "        [373., 215.],\n",
       "        [371., 222.],\n",
       "        [375., 229.],\n",
       "        [399., 230.],\n",
       "        [388., 217.],\n",
       "        [385., 221.],\n",
       "        [388., 227.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVideoDataset(Dataset):\n",
    "    def __init__(self, video_frames_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_frames_path (str): Path to the folder containing video frames.\n",
    "        \"\"\"\n",
    "        self.video_frames_path = video_frames_path\n",
    "\n",
    "        # Transform for resizing and normalizing frames\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # Resize to ResNet input size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "        ])\n",
    "\n",
    "        # Load frames\n",
    "        self.frames = self._load_frames()\n",
    "\n",
    "    def _load_frames(self):\n",
    "        \"\"\"Load all frames from the video frames directory.\"\"\"\n",
    "        frame_files = sorted(os.listdir(self.video_frames_path))  # Sort by frame order\n",
    "        frames = [\n",
    "            cv2.imread(os.path.join(self.video_frames_path, frame))\n",
    "            for frame in frame_files\n",
    "        ]\n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of frames.\"\"\"\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return 5-frame sequences.\"\"\"\n",
    "        sequence_frames = []\n",
    "\n",
    "        for offset in range(-2, 3):  # Generate a sequence of 5 frames\n",
    "            target_idx = idx + offset\n",
    "            if target_idx < 0 or target_idx >= len(self.frames):\n",
    "                # Zero padding for out-of-bounds indices\n",
    "                empty_frame = torch.zeros((3, 224, 224))\n",
    "                sequence_frames.append(empty_frame)\n",
    "            else:\n",
    "                frame = self.transform(self.frames[target_idx])\n",
    "                sequence_frames.append(frame)\n",
    "\n",
    "        frames = torch.stack(sequence_frames)  # Shape: (5, 3, 224, 224)\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Frames Shape: torch.Size([2, 5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "video_frames_path = r\"C:\\Users\\abdul\\Desktop\\visionrd\\AI-Hackathon24\\data\\gtea_png\\gtea_png\\png\\S1_Cheese_C1\"\n",
    "\n",
    "dataset = SimpleVideoDataset(video_frames_path)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)  # Batch size 2 for demonstration\n",
    "\n",
    "# Iterate over the dataset\n",
    "for batch_idx, frames in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(f\"Frames Shape: {frames.shape}\")  # Shape: (batch_size, 5, 3, 224, 224)\n",
    "    break  # Only one batch for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
